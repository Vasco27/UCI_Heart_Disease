{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Help functions\n",
    "to_numeric = function(x) {\n",
    "    return(as.numeric(as.character(x)))\n",
    "}\n",
    "#Splits dataset into a train and test set with given percent split\n",
    "train_test_split = function(data, train_size = 0.7) {\n",
    "    set.seed(1)\n",
    "    \n",
    "    #train_idx = createDataPartition(data$target, p = train_size, list = FALSE, times = 1)\n",
    "    train_idx = sample(1:nrow(data), train_size*nrow(data))\n",
    "    \n",
    "    tr = data[train_idx, ]\n",
    "    te = data[-train_idx, ]\n",
    "    \n",
    "    return(list(\"train\" = tr, \"test\" = te))\n",
    "}\n",
    "\n",
    "#Make logistic regression predictions, can be changed to return probabilities or it can be used to predict lasso and ridge regression\n",
    "make_preds = function(model, data.test, threshold = 0.5, probs = FALSE, feature.selection = FALSE) {\n",
    "    if(feature.selection == TRUE) {\n",
    "        glm.probs = predict(model, newx = model.matrix(target ~ ., data = data.test)[, -1], type = \"response\")\n",
    "    } else {\n",
    "        glm.probs = predict(model, newdata = data.test, type = \"response\")   \n",
    "    }\n",
    "    \n",
    "    if(probs == TRUE) {\n",
    "        return(glm.probs)\n",
    "    }\n",
    "    \n",
    "    glm.preds = ifelse(glm.probs > threshold, 1, 0)\n",
    "    return(factor(glm.preds)) #deixa de ser preciso o y.test\n",
    "}\n",
    "\n",
    "#Predict for the lda and qda models\n",
    "lda_preds = function(model, x.test, threshold = 0.5, probs = FALSE) {\n",
    "    lda.pred = predict(model, x.test)\n",
    "    lda.probs = lda.pred$posterior[, 2] #probs of being 1 (positive)\n",
    "    \n",
    "    if(probs == TRUE) {\n",
    "        return(lda.probs)\n",
    "    }\n",
    "    \n",
    "    if(threshold == 0.5) {\n",
    "        return(lda.pred$class)\n",
    "    } else {\n",
    "        return(factor(ifelse(lda.probs > threshold, 1, 0)))\n",
    "    }    \n",
    "}\n",
    "\n",
    "#Evaluate the various models passed as parameters, returns the metrics (Should be able to receive a function like \"make_preds\" for each type of model)\n",
    "#If feature.selection = TRUE, models.reduced contains the feature selection models\n",
    "evaluate_models = function(models.complete, models.reduced, predict_function = make_preds, best.thresholds = FALSE, tpr.threshold = 0.8, fpr.threshold = 0.3,\n",
    "                           feature.selection = FALSE, plot.width = 24, plot.height = 12, title = \"Different metrics for each model\", \n",
    "                           x.models = c(\"Complete\", \"Complete_step\", \"Reduced\", \"Reduced_step\")) {\n",
    "    \n",
    "    #train test spliting for both types of datasets\n",
    "    ret = train_test_split(uci_heart, train_size = 0.7); train.complete = ret$train; test.complete = ret$test\n",
    "    if(feature.selection == FALSE) {\n",
    "        ret = train_test_split(df.reduced, train_size = 0.7); train.reduced = ret$train ;test.reduced = ret$test\n",
    "    }\n",
    "    #It is the same for both datasets\n",
    "    y_train = train.complete$target; y_test = test.complete$target\n",
    "    \n",
    "    #define thresholds\n",
    "    if(best.thresholds == FALSE) {\n",
    "        thresholds = c(0.5, 0.5, 0.5, 0.5)\n",
    "    } else {\n",
    "        thresholds = best_thresholds(models.complete, models.reduced, predict_function, test.complete, test.reduced, y_test, tpr.thresh = tpr.threshold, fpr.thresh = fpr.threshold)\n",
    "    }\n",
    "\n",
    "    #Evaluate model and retrieve immportant metrics\n",
    "    recalls = c(); precisions = c(); f1scores = c(); specificities = c(); accs = c(); AUCs = c(); type = c(); i = 1\n",
    "    for(model in models.complete){\n",
    "        preds = predict_function(model, test.complete, threshold = thresholds[i])\n",
    "        confm = caret::confusionMatrix(preds, y_test, positive = \"1\")\n",
    "        recalls[i] = confm$byClass[\"Recall\"]; precisions[i] = confm$byClass[\"Precision\"]; f1scores[i] = confm$byClass[\"F1\"]; specificities[i] = confm$byClass[\"Specificity\"]; accs[i] = confm$overall[\"Accuracy\"];\n",
    "        probs = predict_function(model, test.complete, threshold = thresholds[i], probs = TRUE)\n",
    "        AUCs[i] = calc_AUC(probs, y_test)$auc\n",
    "        type[i] = \"Complete\"\n",
    "\n",
    "        i = i + 1\n",
    "    }\n",
    "\n",
    "    for(model in models.reduced) {\n",
    "        if(feature.selection == TRUE) {\n",
    "            preds = predict_function(model, test.complete, threshold = thresholds[i], feature.selection = TRUE)\n",
    "            probs = predict_function(model, test.complete, threshold = thresholds[i], feature.selection = TRUE, probs = TRUE)\n",
    "            type[i] = \"Feature Selection\"\n",
    "        } else {\n",
    "            preds = predict_function(model, test.reduced, threshold = thresholds[i])\n",
    "            probs = predict_function(model, test.reduced, threshold = thresholds[i], probs = TRUE)\n",
    "            type[i] = \"Reduced\"\n",
    "        }\n",
    "        \n",
    "        confm = caret::confusionMatrix(preds, y_test, positive = \"1\")\n",
    "        recalls[i] = confm$byClass[\"Recall\"]; precisions[i] = confm$byClass[\"Precision\"]; f1scores[i] = confm$byClass[\"F1\"]; specificities[i] = confm$byClass[\"Specificity\"]; accs[i] = confm$overall[\"Accuracy\"];\n",
    "        AUCs[i] = calc_AUC(probs, y_test)$auc\n",
    "\n",
    "        i = i + 1\n",
    "    }\n",
    "\n",
    "    metrics = data.frame(Recall = recalls, Precision = precisions, F1 = f1scores, Specificity = specificities, Accuracy = accs, AUC = AUCs, Type = type, Models = x.models, row.names = NULL)\n",
    "    \n",
    "    #visualization\n",
    "    #Uses thresholds?\n",
    "    if(thresholds[1] != 0.5) {\n",
    "        subtitle = make_subtitle(thresholds, x.models)\n",
    "        print(visualize_metrics(metrics, plot.width, plot.height, plot.title = title, plot.subtitle = subtitle))\n",
    "    } else {\n",
    "        print(visualize_metrics(metrics, plot.width, plot.height, plot.title = title))\n",
    "    }\n",
    "    \n",
    "    return(metrics)\n",
    "}\n",
    "\n",
    "#Method to Visualize all the metrics of a model, the metrics dataframe must have the structure [Metrics, Model Type, Model Name]\n",
    "visualize_metrics = function(metrics, plot.width = 24, plot.height = 12, plot.title = \"Different metrics for each model\", plot.subtitle = \"Threshold = 0.5\") {\n",
    "    options(repr.plot.width = plot.width, repr.plot.height = plot.height)\n",
    "    #melt for facet wrap\n",
    "    metrics.melt = melt(metrics, id.vars = c(\"Models\", \"Type\"), value.name = \"Count\", variable.name = \"Variable\")\n",
    "\n",
    "    p = ggplot(data = metrics.melt, aes(x = Models, y = Count)) + \n",
    "            geom_bar(aes(fill = Type), stat = \"identity\", width = 0.5, position = \"dodge\") + facet_wrap(\"Variable\") +\n",
    "            geom_text(aes(label = round(Count, 2)), position = position_dodge(width = 0.9), vjust = 1.25, size = 10) + \n",
    "            scale_y_continuous(limits = c(0.55, 0.95), oob=rescale_none) + \n",
    "            labs(title = plot.title, subtitle = plot.subtitle, y = \"Metric Percentage\") +\n",
    "            theme(text = element_text(size = 20), plot.title = element_text(size = 30, face = \"bold\", hjust = 0.5), panel.grid.minor = element_blank(), \n",
    "                  panel.grid.major = element_blank(), axis.text.x = element_text(angle = 45, vjust = 0.75))\n",
    "    \n",
    "    return(p)\n",
    "}\n",
    "\n",
    "#Make subtitle for the metrics plot, based on thresholds\n",
    "make_subtitle = function(thresh, x.models) {\n",
    "    s = \"\"\n",
    "    for(i in 1:length(thresh)) {\n",
    "        s = paste(s, x.models[i], \"threshold =\", thresh[i], \"\\n\")\n",
    "    }\n",
    "    return(s)\n",
    "}\n",
    "\n",
    "#Plots the ROC Curve of all the models in the parameters \"models.complete\" and \"models.reduced\". \n",
    "#\"x.models\" must have the same size as the number of models passed.\n",
    "plot_ROCCurve = function(models.complete, models.reduced, predict_function = make_preds, x.models = c(\"Complete\", \"Complete_step\", \"Reduced\", \"Reduced_step\"), plots.display = c(2,2), plot.width = 24, plot.height = 12) {\n",
    "    options(repr.plot.width = plot.width, repr.plot.height = plot.height)\n",
    "    par(cex.main = 1.75, cex.axis = 1.35, cex.lab = 1.5, lwd = 2.5, mfrow = plots.display)\n",
    "    \n",
    "    #train test spliting for both types of datasets\n",
    "    ret = train_test_split(uci_heart, train_size = 0.7); train.complete = ret$train; test.complete = ret$test\n",
    "    ret = train_test_split(df.reduced, train_size = 0.7); train.reduced = ret$train ;test.reduced = ret$test\n",
    "    #It is the same for both datasets\n",
    "    y_train = train.reduced$target; y_test = test.reduced$target\n",
    "    \n",
    "    i = 1\n",
    "    for(model in models.complete) {\n",
    "        ret = calc_AUC(predict_function(model, test.complete, probs = TRUE), y_test)\n",
    "        \n",
    "        perf <- performance(ret$pred,\"tpr\",\"fpr\")\n",
    "        plot(perf, colorize = TRUE, main = paste(\"ROC Curve for\", x.models[i], \"\\nAUC =\", ret$auc))\n",
    "        i = i + 1\n",
    "    }\n",
    "    \n",
    "    for(model in models.reduced) {\n",
    "        ret = calc_AUC(predict_function(model, test.reduced, probs = TRUE), y_test)\n",
    "        \n",
    "        perf <- performance(ret$pred,\"tpr\",\"fpr\")\n",
    "        plot(perf, colorize = TRUE, main = paste(\"ROC Curve for\", x.models[i], \"\\nAUC =\", ret$auc))\n",
    "\n",
    "        i = i + 1\n",
    "    }\n",
    "}\n",
    "\n",
    "#Calculate the AUC using the ROCR package\n",
    "calc_AUC = function(y.pred, y.test) {\n",
    "    pred <- prediction(to_numeric(y.pred), y.test)\n",
    "    auc.tmp = performance(pred, \"auc\")\n",
    "    return(list(\"auc\" = as.numeric(auc.tmp@y.values), \"pred\" = pred))\n",
    "}\n",
    "\n",
    "#Best prediction probabilities threhsoçld, according to the ROC curve cutoffs\n",
    "roc_cutoff = function(predict_function = make_preds, model, x.test, y.test, tpr.threshold = 0.8, fpr.threshold = 0.2) {\n",
    "    pred = calc_AUC(predict_function(model, x.test, probs = TRUE), y.test)$pred\n",
    "    perf = performance(pred, \"tpr\", \"fpr\")\n",
    "    \n",
    "    cutoffs = data.frame(cutoff = perf@alpha.values[[1]], fpr = perf@x.values[[1]], tpr = perf@y.values[[1]]) #get the cutoff, tpr and fpr values\n",
    "    cutoffs = cutoffs[order(cutoffs$tpr, decreasing = TRUE), ] #The best cutoff for the max tpr will be the first\n",
    "    \n",
    "    return(cutoffs[(cutoffs$tpr >= tpr.threshold) & (cutoffs$fpr <= fpr.threshold), ])\n",
    "}\n",
    "\n",
    "#This method computes the best thresholds for a set of models, that improve the recall the most.\n",
    "best_thresholds = function(models.complete, models.reduced, predict_function = make_preds, test.complete, test.reduced, y.test, tpr.thresh = 0.8, fpr.thresh = 0.3) {\n",
    "    thresh = c(); i = 1\n",
    "\n",
    "    for(model in models.complete) {\n",
    "        temp = roc_cutoff(predict_function, model, test.complete, y.test, tpr.threshold = tpr.thresh, fpr.threshold = fpr.thresh) #chose the acceptable fpr\n",
    "        thresh[i] = round(temp$cutoff[1], 2)\n",
    "        i = i + 1\n",
    "    }\n",
    "\n",
    "    for(model in models.red) {\n",
    "        temp = roc_cutoff(predict_function, model, test.reduced, y.test, tpr.threshold = tpr.thresh, fpr.threshold = fpr.thresh)\n",
    "        thresh[i] = round(temp$cutoff[1], 2)\n",
    "        i = i + 1\n",
    "    }\n",
    "    \n",
    "    return(thresh)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FOR THE KNN MODELS-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Evaluates KNN method according to the K parameter of the method. Returns the relevant metrics for the model\n",
    "evaluate_knn = function(ks = c(1,3,5,7,9,11,15), x.models = c(\"Complete model\", \"Reduced model\")) {\n",
    "    #train test spliting for both types of datasets\n",
    "    ret = train_test_split(uci_heart, train_size = 0.7); train.complete = ret$train; test.complete = ret$test\n",
    "    ret = train_test_split(df.reduced, train_size = 0.7); train.reduced = ret$train ;test.reduced = ret$test\n",
    "    \n",
    "    #It is the same for both datasets\n",
    "    y_train = train.complete$target; y_test = test.complete$target\n",
    "    \n",
    "    #Evaluate model and retrieve immportant metrics\n",
    "    set.seed(1)\n",
    "    recalls = c(); precisions = c(); f1scores = c(); specificities = c(); accs = c(); i = 1\n",
    "    for(k in ks) {\n",
    "        knn.preds = knn(train.complete, test.complete, cl = y_train, k = k)\n",
    "        confm = caret::confusionMatrix(knn.preds, y_test, positive = \"1\")\n",
    "            \n",
    "        recalls[i] = confm$byClass[\"Recall\"]; precisions[i] = confm$byClass[\"Precision\"]; f1scores[i] = confm$byClass[\"F1\"]; specificities[i] = confm$byClass[\"Specificity\"]\n",
    "        accs[i] = confm$overall[\"Accuracy\"]\n",
    "        i = i + 1\n",
    "    }\n",
    "    \n",
    "    metrics.complete = data.frame(Recall = recalls, Precision = precisions, F1 = f1scores, Specificity = specificities, Accuracy = accs, k = ks, row.names = NULL)\n",
    "    i = 1\n",
    "    for(k in ks) {\n",
    "        knn.preds = knn(train.reduced, test.reduced, cl = y_train, k = k)\n",
    "        confm = caret::confusionMatrix(knn.preds, y_test, positive = \"1\")\n",
    "            \n",
    "        recalls[i] = confm$byClass[\"Recall\"]; precisions[i] = confm$byClass[\"Precision\"]; f1scores[i] = confm$byClass[\"F1\"]; specificities[i] = confm$byClass[\"Specificity\"]\n",
    "        accs[i] = confm$overall[\"Accuracy\"]\n",
    "        i = i + 1\n",
    "    }\n",
    "    \n",
    "    metrics.reduced = data.frame(Recall = recalls, Precision = precisions, F1 = f1scores, Specificity = specificities, Accuracy = accs, k = ks, row.names = NULL)\n",
    "    \n",
    "    #Visualization (arranged grid)\n",
    "    p1 = visualize_knn(metrics.complete, plot.title = x.models[1])\n",
    "    p2 = visualize_knn(metrics.reduced, plot.title = x.models[2])\n",
    "    \n",
    "    fig = ggarrange(p1, p2, nrow = 1, ncol = 2, common.legend = TRUE, legend = \"right\") #one common legend between the plots\n",
    "    grid = annotate_figure(fig,\n",
    "                        top = textGrob(\"Metric values for the KNN method\", gp = gpar(fontsize = 40, fontface = \"bold\", col = \"darkred\"))) #add a title to the grid\n",
    "    print(grid)\n",
    "    \n",
    "    return(list(\"complete\" = metrics.complete, \"reduced\" = metrics.reduced))\n",
    "}\n",
    "\n",
    "#Transforms the metrics dataset into plotable data. Plots the evolution of the metrics as the parameter K increases.\n",
    "visualize_knn = function(metrics, plot.title = \"Metric values for KNN\") {\n",
    "    #melt the dataset to use with ggplot\n",
    "    metrics.melt = melt(metrics, id.vars = c(\"k\"), value.name = \"Count\", variable.name = \"Variable\")\n",
    "    \n",
    "    p = ggplot(data = metrics.melt, aes(x = k, y = Count)) + \n",
    "            geom_line(aes(color = Variable), stat = \"identity\", size = 1) +\n",
    "            scale_x_discrete(limits = k, labels = k) +\n",
    "            scale_y_continuous(limits = c(0.3, 1)) +\n",
    "            scale_color_brewer(palette = \"Set1\", name = \"Metrics\") +\n",
    "            labs(title = plot.title, y = \"Metric Percentage\") +\n",
    "                    theme(text = element_text(size = 20), plot.title = element_text(size = 30, face = \"bold\", hjust = 0.5, color = \"darkorange\"), panel.grid.minor = element_blank(), \n",
    "                          panel.grid.major = element_blank())\n",
    "    \n",
    "    return(p)\n",
    "}\n",
    "\n",
    "#Visualization\n",
    "visualize_best_k = function(metrics.list, best.ks = c(5,5), x.models = c(\"Complete\", \"Reduced\")) {\n",
    "    metrics.complete = metrics.list$complete\n",
    "    metrics.reduced = metrics.list$reduced\n",
    "    \n",
    "    best.metrics.complete = metrics.complete[metrics.complete$k == best.ks[1], ]\n",
    "    best.metrics.reduced = metrics.reduced[metrics.reduced$k == best.ks[2], ]\n",
    "\n",
    "    best.metrics = rbind(best.metrics.complete, best.metrics.reduced)\n",
    "    best.metrics$Type = x.models\n",
    "    best.metrics$k = as.character(best.metrics$k) #Make it a discrete value\n",
    "\n",
    "    best.metrics.melt = melt(best.metrics, id.vars = c(\"k\", \"Type\"), value.name = \"Count\", variable.name = \"Variable\")\n",
    "\n",
    "    p = ggplot(data = best.metrics.melt, aes(x = Type, y = Count)) +\n",
    "            geom_bar(aes(fill = k), stat = \"identity\", position = \"dodge\", width = 0.5) + facet_wrap(\"Variable\") +\n",
    "            geom_text(aes(label = round(Count, 2)), position = position_dodge(width = 0.9), vjust = 1.25, size = 10) + \n",
    "            scale_y_continuous(limits = c(0.55, 0.95), oob=rescale_none) + \n",
    "            labs(title = \"Best KNN metrics\", y = \"Metric Percentage\") +\n",
    "            theme(text = element_text(size = 20), plot.title = element_text(size = 30, face = \"bold\", hjust = 0.5), panel.grid.minor = element_blank(), \n",
    "                    panel.grid.major = element_blank())\n",
    "    \n",
    "    print(p)\n",
    "    \n",
    "    return(best.metrics.melt)\n",
    "}\n",
    "\n",
    "#Evaluating and plotting the train and test errors of the different models for the KNN method\n",
    "evaluate_knn_traintest_errors = function(k, x.models = c(\"Complete model\", \"Reduced model\")) {\n",
    "    set.seed(1)\n",
    "    knn.complete.trainerr = c(); knn.reduced.trainerr = c(); knn.complete.testerr = c(); knn.reduced.testerr = c(); i = 1\n",
    "    for(param in k) {\n",
    "        preds = knn(train.complete, train.complete, cl = y_train, k = param)\n",
    "        knn.complete.trainerr[i] = 1 - mean(preds == y_train)\n",
    "\n",
    "        preds = knn(train.complete, test.complete, cl = y_train, k = param)\n",
    "        knn.complete.testerr[i] = 1 - mean(preds == y_test)\n",
    "\n",
    "        preds = knn(train.reduced, train.reduced, cl = y_train, k = param)\n",
    "        knn.reduced.trainerr[i] = 1 - mean(preds == y_train)\n",
    "\n",
    "        preds = knn(train.reduced, test.reduced, cl = y_train, k = param)\n",
    "        knn.reduced.testerr[i] = 1 - mean(preds == y_test)\n",
    "\n",
    "        i = i + 1\n",
    "    }\n",
    "\n",
    "    knn.complete.err = data.frame(cbind(knn.complete.trainerr, knn.complete.testerr))\n",
    "    knn.reduced.err = data.frame(cbind(knn.reduced.trainerr, knn.reduced.testerr))\n",
    "    \n",
    "    p1 = plot_knn_traintest_errors(knn.complete.err, k, x.models[1])\n",
    "    p2 = plot_knn_traintest_errors(knn.reduced.err, k, x.models[2])\n",
    "    \n",
    "    fig = ggarrange(p1, p2, nrow = 1, ncol = 2, common.legend = TRUE, legend = \"right\") #one common legend between the plots\n",
    "    grid = annotate_figure(fig,\n",
    "                        top = textGrob(\"Train vs Test errors for the KNN method\", gp = gpar(fontsize = 40, fontface = \"bold\", col = \"darkred\"))) #add a title to the grid\n",
    "    print(grid)\n",
    "    \n",
    "    return(list(\"complete_err\" = knn.complete.err, \"reduced_err\" = knn.reduced.err))\n",
    "}\n",
    "\n",
    "#Visualization of the train and test errors for the KNN method\n",
    "plot_knn_traintest_errors = function(model.errors, ks, plot.title) {\n",
    "    colnames(model.errors) = c(\"train error\", \"test error\")\n",
    "    model.errors$k = ks\n",
    "    model.errors.melt = melt(model.errors, id.vars = \"k\", value.name = \"Count\", variable.name = \"Variable\")\n",
    "\n",
    "\n",
    "    p = ggplot(data = model.errors.melt, aes(x = k, y = Count)) + \n",
    "            geom_line(aes(color = Variable), stat = \"identity\", size = 1) +\n",
    "                    scale_x_discrete(limits = k, labels = k) +\n",
    "                    scale_y_continuous(limits = c(0, 0.4)) +\n",
    "                    scale_color_brewer(palette = \"Set1\", name = \"Train/Test errors\") +\n",
    "                    labs(title = plot.title, y = \"Error percentage\") +\n",
    "                        theme(text = element_text(size = 20), plot.title = element_text(size = 30, face = \"bold\", hjust = 0.5, color = \"darkorange\"), panel.grid.minor = element_blank(), \n",
    "                                  panel.grid.major = element_blank())\n",
    "    \n",
    "    return(p)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in system3(\"jupyter\", args, capture = quiet, env = pythonpath):\n",
      "\"Ignoring env argument, as R does not support it on windows\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'C:/Users/HP/Desktop/mestrado/2semestre/EstatisticaEAnaliseDeDados/EADProject/EADRepo/Help_Functions.ipynb'"
      ],
      "text/latex": [
       "'C:/Users/HP/Desktop/mestrado/2semestre/EstatisticaEAnaliseDeDados/EADProject/EADRepo/Help\\_Functions.ipynb'"
      ],
      "text/markdown": [
       "'C:/Users/HP/Desktop/mestrado/2semestre/EstatisticaEAnaliseDeDados/EADProject/EADRepo/Help_Functions.ipynb'"
      ],
      "text/plain": [
       "[1] \"C:/Users/HP/Desktop/mestrado/2semestre/EstatisticaEAnaliseDeDados/EADProject/EADRepo/Help_Functions.ipynb\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(nbconvertR)\n",
    "nbconvert(\n",
    "    \"Help_Functions.ipynb\",\n",
    "    fmt = \"script\",\n",
    "    quiet = TRUE,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
