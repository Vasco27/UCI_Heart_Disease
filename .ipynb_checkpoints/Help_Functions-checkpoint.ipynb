{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Help functions\n",
    "to_numeric = function(x) {\n",
    "    return(as.numeric(as.character(x)))\n",
    "}\n",
    "#Splits dataset into a train and test set with given percent split\n",
    "train_test_split = function(data, train_size = 0.7) {\n",
    "    set.seed(1)\n",
    "    \n",
    "    #train_idx = createDataPartition(data$target, p = train_size, list = FALSE, times = 1)\n",
    "    train_idx = sample(1:nrow(data), train_size*nrow(data))\n",
    "    \n",
    "    tr = data[train_idx, ]\n",
    "    te = data[-train_idx, ]\n",
    "    \n",
    "    return(list(\"train\" = tr, \"test\" = te))\n",
    "}\n",
    "\n",
    "#Make logistic regression predictions, can be changed to return probabilities or it can be used to predict lasso and ridge regression\n",
    "make_preds = function(model, data.test, threshold = 0.5, probs = FALSE, feature.selection = FALSE) {\n",
    "    if(feature.selection == TRUE) {\n",
    "        glm.probs = predict(model, newx = model.matrix(target ~ ., data = data.test)[, -1], type = \"response\")\n",
    "    } else {\n",
    "        glm.probs = predict(model, newdata = data.test, type = \"response\")   \n",
    "    }\n",
    "    \n",
    "    if(probs == TRUE) {\n",
    "        return(glm.probs)\n",
    "    }\n",
    "    \n",
    "    glm.preds = ifelse(glm.probs > threshold, 1, 0)\n",
    "    return(factor(glm.preds)) #deixa de ser preciso o y.test\n",
    "}\n",
    "\n",
    "#Predict for the lda model\n",
    "lda_preds = function(model, x.test, probs = FALSE) {\n",
    "    lda.pred = predict(model, x.test)\n",
    "    \n",
    "    if(probs == TRUE) {\n",
    "        return(lda.pred$posterior[, 2]) #probs of being 1 (positive)\n",
    "    }\n",
    "    \n",
    "    return(lda.pred$class)\n",
    "}\n",
    "\n",
    "#Evaluate the various models passed as parameters, returns the metrics (Should be able to receive a function like \"make_preds\" for each type of model)\n",
    "#If feature.selection = TRUE, models.reduced contains the feature selection models\n",
    "evaluate_models = function(models.complete, models.reduced, predict_function = make_preds, feature.selection = FALSE, plot.width = 24, plot.height = 12, x.models = c(\"Complete\", \"Complete_step\", \"Reduced\", \"Reduced_step\")) {\n",
    "    \n",
    "    #train test spliting for both types of datasets\n",
    "    ret = train_test_split(uci_heart, train_size = 0.7); train.complete = ret$train; test.complete = ret$test\n",
    "    if(feature.selection == FALSE) {\n",
    "        ret = train_test_split(df.reduced, train_size = 0.7); train.reduced = ret$train ;test.reduced = ret$test\n",
    "    }\n",
    "    #It is the same for both datasets\n",
    "    y_train = train.complete$target; y_test = test.complete$target\n",
    "\n",
    "    #Evaluate model and retrieve immportant metrics\n",
    "    recalls = c(); precisions = c(); f1scores = c(); specificities = c(); accs = c(); AUCs = c(); type = c()\n",
    "\n",
    "    i = 1\n",
    "    for(model in models.complete){\n",
    "        preds = predict_function(model, test.complete)\n",
    "        confm = caret::confusionMatrix(preds, y_test, positive = \"1\")\n",
    "        recalls[i] = confm$byClass[\"Recall\"]; precisions[i] = confm$byClass[\"Precision\"]; f1scores[i] = confm$byClass[\"F1\"]; specificities[i] = confm$byClass[\"Specificity\"]; accs[i] = confm$overall[\"Accuracy\"];\n",
    "        probs = predict_function(model, test.complete, probs = TRUE)\n",
    "        AUCs[i] = calc_AUC(probs, y_test)$auc\n",
    "        type[i] = \"Complete\"\n",
    "\n",
    "        i = i + 1\n",
    "    }\n",
    "\n",
    "    for(model in models.reduced) {\n",
    "        if(feature.selection == TRUE) {\n",
    "            preds = predict_function(model, test.complete, feature.selection = TRUE)\n",
    "            probs = predict_function(model, test.complete, feature.selection = TRUE, probs = TRUE)\n",
    "            type[i] = \"Feature Selection\"\n",
    "        } else {\n",
    "            preds = predict_function(model, test.reduced)\n",
    "            probs = predict_function(model, test.reduced, probs = TRUE)\n",
    "            type[i] = \"Reduced\"\n",
    "        }\n",
    "        \n",
    "        confm = caret::confusionMatrix(preds, y_test, positive = \"1\")\n",
    "        recalls[i] = confm$byClass[\"Recall\"]; precisions[i] = confm$byClass[\"Precision\"]; f1scores[i] = confm$byClass[\"F1\"]; specificities[i] = confm$byClass[\"Specificity\"]; accs[i] = confm$overall[\"Accuracy\"];\n",
    "        AUCs[i] = calc_AUC(probs, y_test)$auc\n",
    "\n",
    "        i = i + 1\n",
    "    }\n",
    "\n",
    "    metrics = data.frame(Recall = recalls, Precision = precisions, F1 = f1scores, Specificity = specificities, Accuracy = accs, AUC = AUCs, Type = type, Models = x.models, row.names = NULL)\n",
    "    \n",
    "    #visualization\n",
    "    print(visualize_metrics(metrics, plot.width, plot.height))\n",
    "    \n",
    "    return(metrics)\n",
    "}\n",
    "\n",
    "#Method to Visualize all the metrics of a model, the metrics dataframe must have the structure [Metrics, Model Type, Model Name]\n",
    "visualize_metrics = function(metrics, plot.width = 24, plot.height = 12) {\n",
    "    options(repr.plot.width = plot.width, repr.plot.height = plot.height)\n",
    "    #melt for facet wrap\n",
    "    metrics.melt = melt(metrics, id.vars = c(\"Models\", \"Type\"), value.name = \"Count\", variable.name = \"Variable\")\n",
    "\n",
    "    p = ggplot(data = metrics.melt, aes(x = Models, y = Count)) + \n",
    "            geom_bar(aes(fill = Type), stat = \"identity\", width = 0.5, position = \"dodge\") + facet_wrap(\"Variable\") +\n",
    "            geom_text(aes(label = round(Count, 2)), position = position_dodge(width = 0.9), vjust = 1.25, size = 10) + \n",
    "            scale_y_continuous(limits = c(0.65, 0.95), oob=rescale_none) + \n",
    "            labs(title = \"Different metrics for each model\", y = \"Metric Percentage\") +\n",
    "            theme(text = element_text(size = 20), plot.title = element_text(size = 30, face = \"bold\", hjust = 0.5), panel.grid.minor = element_blank(), \n",
    "                  panel.grid.major = element_blank(), axis.text.x = element_text(angle = 45, vjust = 0.75))\n",
    "    \n",
    "    return(p)\n",
    "}\n",
    "\n",
    "#Plots the ROC Curve of all the models in the parameters \"models.complete\" and \"models.reduced\". \n",
    "#\"x.models\" must have the same size as the number of models passed.\n",
    "plot_ROCCurve = function(models.complete, models.reduced, predict_function = make_preds, x.models = c(\"Complete\", \"Complete_step\", \"Reduced\", \"Reduced_step\"), plots.display = c(2,2), plot.width = 24, plot.height = 12) {\n",
    "    options(repr.plot.width = plot.width, repr.plot.height = plot.height)\n",
    "    par(cex.main = 1.75, cex.axis = 1.35, cex.lab = 1.5, lwd = 2.5, mfrow = plots.display)\n",
    "    \n",
    "    #train test spliting for both types of datasets\n",
    "    ret = train_test_split(uci_heart, train_size = 0.7); train.complete = ret$train; test.complete = ret$test\n",
    "    ret = train_test_split(df.reduced, train_size = 0.7); train.reduced = ret$train ;test.reduced = ret$test\n",
    "    #It is the same for both datasets\n",
    "    y_train = train.reduced$target; y_test = test.reduced$target\n",
    "    \n",
    "    i = 1\n",
    "    for(model in models.complete) {\n",
    "        ret = calc_AUC(predict_function(model, test.complete, probs = TRUE), y_test)\n",
    "        \n",
    "        perf <- performance(ret$pred,\"tpr\",\"fpr\")\n",
    "        plot(perf, colorize = TRUE, main = paste(\"ROC Curve for\", x.models[i], \"\\nAUC =\", ret$auc))\n",
    "        i = i + 1\n",
    "    }\n",
    "    \n",
    "    for(model in models.reduced) {\n",
    "        ret = calc_AUC(predict_function(model, test.reduced, probs = TRUE), y_test)\n",
    "        \n",
    "        perf <- performance(ret$pred,\"tpr\",\"fpr\")\n",
    "        plot(perf, colorize = TRUE, main = paste(\"ROC Curve for\", x.models[i], \"\\nAUC =\", ret$auc))\n",
    "\n",
    "        i = i + 1\n",
    "    }\n",
    "}\n",
    "\n",
    "#Calculate the AUC using the ROCR package\n",
    "calc_AUC = function(y.pred, y.test) {\n",
    "    pred <- prediction(to_numeric(y.pred), y.test)\n",
    "    auc.tmp = performance(pred, \"auc\")\n",
    "    return(list(\"auc\" = as.numeric(auc.tmp@y.values), \"pred\" = pred))\n",
    "}\n",
    "\n",
    "#Best prediction probabilities threhsoÃ§ld, according to the ROC curve cutoffs\n",
    "roc_cutoff = function(predict_function = make_preds, model, x.test, y.test, tpr.threshold = 0.8, fpr.threshold = 0.2) {\n",
    "    pred = calc_AUC(predict_function(model, x.test, probs = TRUE), y.test)$pred\n",
    "    perf = performance(pred, \"tpr\", \"fpr\")\n",
    "    \n",
    "    cutoffs = data.frame(cutoff = perf@alpha.values[[1]], fpr = perf@x.values[[1]], tpr = perf@y.values[[1]]) #get the cutoff, tpr and fpr values\n",
    "    cutoffs = cutoffs[order(cutoffs$tpr, decreasing = TRUE), ] #The best cutoff for the max tpr will be the first\n",
    "    \n",
    "    return(cutoffs[(cutoffs$tpr >= tpr.threshold) & (cutoffs$fpr <= fpr.threshold), ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in system3(\"jupyter\", args, capture = quiet, env = pythonpath):\n",
      "\"Ignoring env argument, as R does not support it on windows\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'C:/Users/HP/Desktop/mestrado/2semestre/EstatisticaEAnaliseDeDados/EADProject/EADRepo/Help_Functions.ipynb'"
      ],
      "text/latex": [
       "'C:/Users/HP/Desktop/mestrado/2semestre/EstatisticaEAnaliseDeDados/EADProject/EADRepo/Help\\_Functions.ipynb'"
      ],
      "text/markdown": [
       "'C:/Users/HP/Desktop/mestrado/2semestre/EstatisticaEAnaliseDeDados/EADProject/EADRepo/Help_Functions.ipynb'"
      ],
      "text/plain": [
       "[1] \"C:/Users/HP/Desktop/mestrado/2semestre/EstatisticaEAnaliseDeDados/EADProject/EADRepo/Help_Functions.ipynb\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(nbconvertR)\n",
    "nbconvert(\n",
    "    \"Help_Functions.ipynb\",\n",
    "    fmt = \"script\",\n",
    "    quiet = TRUE,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
